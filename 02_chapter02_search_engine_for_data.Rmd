---
output: html_document
---

# The features of a modern data dissemination platform {#chapter02}

In the introductory section of this Guide, we proposed that modern data dissemination platforms should be modeled after the most successful e-commerce platforms. These platforms are optimized to serve both buyers (in our context, the data users) and sellers (in our context, the data providers) in the most efficient manner. Internet search engines, and in particular the way they exploit the capabilities of generative artificial intelligence, are also a useful source of inspiration for the design of data catalogs. In this chapter, we outline features that a modern online data catalog should incorporate to adhere to these models.

We provide recommendations for developing data catalogs that encompass lexical search and semantic search, filtering, advanced search functionality, interactive user interfaces, and the capability to operate as useful recommender systems. We approach the topic from three distinct perspectives: the viewpoint of **data users**, who represent a highly diverse community with varying needs, preferences, expectations, and capabilities; the standpoint of **data suppliers**, who either publish their data or delegate the task to a data library; and the perspective of **catalog administrators**, responsible for curating and disseminating data in a responsible, effective, and efficient manner while optimizing both user and supplier satisfaction.

The creation of a data dissemination platform is a collaborative endeavor, engaging data curators, user experience (UX) experts, user interface (UI) designers, search and systems engineers, and subject matter specialists with a profound understanding of both the data and the users' requirements and preferences. Inclusive in this development process should be the active participation of the users, allowing them to provide feedback that directly influences the system's design.

The set of features we describe below are proposed for a large data catalog, developed and mantained by a well-resourced organization. Not all features would be appropriate for a small catalog or for a catalog maintained by an organization facing technical and other constraints. In such situations, semantic search capability and the implementation of features that require the implementation of machine learning of generative artificial intelligence may not be recommended.

## Features for data users 

In order to cultivate a favorable user experience, online data catalogs must offer an intuitive and efficient interface, allowing users to effortlessly access the most pertinent datasets. To meet user expectations effectively, one should emphasize **simplicity** and **relevance**. A data dissemination platform should also be made accessible free of discrimination. The Web Content Accessibility Guidelines (WCAG) international standard provides principles and recommendations for accessibility to people with disabilities. Integrating these principles into the design of data catalogs can deliver a seamless and user-friendly experience, akin to the convenience and ease provided by internet search engines and e-commerce platforms. 

Users of a data catalog will typically want to **search** and **filter** the assets available in a catalog. They will also wantto **browse** and possibly **sort** the catalog entries or the entries returned by running a search or by activating a filter. We describe below how the search, filtering, browsing, and sorting features may be designed to provide a user-friendly experience. **Other features** of a data catalog that may be useful to users are also proposed, including features enabled by artificial intelligence, which are to a large extent in exploratory stage.

As artificial intelligence (AI) and machine learning (ML) solutions continue to advance, there emerges a capacity for these search engines to go beyond merely delivering a ranked list of results. In certain scenarios, particularly when the nature and content of a user's query permit, the search algorithm can generate a more direct and informative response. This could take the form of a conversational reply, synthesized through generative AI, or the presentation of structured data grids and visualizations that directly address the query. For such capabilities to be effectively implemented, the system must exhibit a level of intelligence that allows it to discern the appropriate response strategy. This involves parsing the intricacies of the query, recognizing patterns, and making informed decisions on whether to provide a direct answer or adhere to the conventional approach of returning a ranked list of resources. The fusion of advanced AI and ML technologies with search engines in data catalogs marks a significant evolution, empowering users with more nuanced and contextually relevant information retrieval experiences.

### Search 

Search engines implemented in data catalogs can be categorized as either *lexical* or *semantic*, or they may seamlessly integrate elements of both. A lexical search, also referred to as a keyword or full-text search, operates by matching literal terms from a user query to the search engine's index, subsequently returning datasets whose indexed metadata includes the specified terms. On the other hand, a semantic search strives to pinpoint datasets with metadata that bears semantic resemblance to the query. Ideally, a comprehensive data catalog would incorporate both lexical and semantic search engines, seamlessly blended into a unified search tool. However, the integration of semantic search functionality presents a complexity in implementation.

Semantic search leverages advanced techniques like embeddings, vector indexing, and similarity measures to enhance the precision and relevance of search results by capturing the semantic meaning of user queries and dataset content. The process typically involves converting words or phrases into high-dimensional vectors using embeddings. Embeddings are representations of words or phrases in a continuous vector space, where semantically similar terms are positioned closer to each other.

Vector indexing in specialized *vector databases* is a key aspect of semantic search. These databases, such as Milvus or Qadrant, are optimized for storing and querying high-dimensional vectors efficiently. They enable the indexing of embeddings associated with both user queries and catalog resources, allowing for fast and accurate similarity searches.

Cosine similarity is a common measure used in semantic search to quantify the similarity between vectors. In a vector space, the cosine of the angle between two vectors is calculated, with a higher cosine indicating greater similarity. This measure is instrumental in ranking and retrieving datasets based on their semantic relevance to a user's query.

To facilitate the implementation of semantic search, APIs play a crucial role. APIs for generating embeddings streamline the process by providing access to pre-trained models or embedding services. These APIs take care of the complex task of converting textual data into meaningful embeddings, saving time and resources for developers. Additionally, APIs for search engines allow the integration of semantic search capabilities seamlessly, enabling the retrieval of relevant results based on semantic similarity.

In a search engine, semantic search is often combined with lexical search to offer a comprehensive and effective discovery system. Lexical search involves matching literal terms in a user query to the search engine's index, while semantic search focuses on identifying datasets with semantically similar metadata. Combining these approaches ensures a balanced and robust search system, catering to a wide range of user preferences and query styles.

In summary, semantic search is a powerful approach that involves embeddings, vector indexing, and similarity measures to provide users with more contextually relevant results. Specialized vector databases, APIs for embeddings, and a combination with lexical search contribute to the seamless implementation of semantic search in data catalog search engines, enhancing the overall user experience and discovery capabilities.

#### Search box

In a data catalog, the primary means of searching for data is typically through a single, intuitive search box. Recognizing the diversity in users' search capabilities, the search engine must be designed to accommodate a range of query inputs, acknowledging that not all users will craft perfect queries. 

To enhance user experience and overcome potential spelling errors, the search engine may leverage advanced indexing tools such as Solr or ElasticSearch. Additionally, users should have the flexibility to input queries either through traditional keyboard entry or via audio commands.

<br>
<center>
![image](https://user-images.githubusercontent.com/35276300/229823626-311376be-f75f-4e0b-9e6b-767fa307246b.png)
</center>
<br>

Ensuring a nuanced understanding of user queries, the search engine should employ sophisticated processes, including automated query parsing and enhancement. This parsing mechanism extracts crucial information from the query, discerning factors such as the type of data under consideration, whether the query pertains to specific indicators within the catalog assets, references to geographic locations, time-related elements, and the suitability of a keyword search versus a semantic search. The system also endeavors to detect the language of the query and, if necessary, offers translation services.

A pivotal aspect of the search engine's functionality lies in its ability to decipher the user's intent and return results ranked by relevance. This demands an intricate analysis of the query, potentially leading to immediate answers. Depending on the nature of the query, the application must intelligently determines whether to present results in textual form, organize them in a data grid, or generate visualizations for a more comprehensive understanding. This multifaceted approach ensures that users can interact seamlessly with the data catalog, obtaining the most pertinent information in a manner that aligns with their preferences and requirements.

#### Document as a query

A search engine equipped with semantic search capabilities extends its user-friendly functionality by accommodating queries of varying lengths, including the innovative option for users to submit entire documents—whether in PDF or TXT format—as queries. Upon receiving a document query, the search engine initiates a sophisticated process wherein it meticulously analyzes the semantic content within the document. This involves transforming the document into an embedding vector, a compact representation capturing its semantic essence. Subsequently, the search engine adeptly identifies and retrieves the most relevant resources from the catalog, ensuring a nuanced and contextually rich response to the user's document-based query.

<br>
<center>
![image](https://user-images.githubusercontent.com/35276300/229806674-941ac085-6f6f-45e8-bfa4-d0834cf73587.png)
</center>
<br>

#### Image as a query

A cutting-edge search engine, empowered with advanced image search capabilities, introduces an innovative dimension to user interaction by enabling the submission of images as queries. Users can seamlessly upload images in various formats, unlocking a novel means of exploration. Upon receiving an image query, the search engine embarks on a sophisticated journey, meticulously analyzing the visual content embedded in the image. Through intricate processes such as feature extraction and image embedding, the search engine translates the visual information into a compact and meaningful representation. Leveraging this representation, the search engine then identifies and retrieves the most pertinent resources from the catalog, delivering a tailored and visually aligned response to the user's image-based query. This transformative capability enhances the search experience, allowing users to explore and discover relevant content through the lens of visual input.

#### Suggested queries

Contemporary search engines can further enrich the user experience by the proactive generation of pertinent and refined query suggestions. Beyond merely processing user queries, these advanced search engines leverage sophisticated techniques to automatically propose alternative queries that are more useful and relevant. This intelligent feature is often implemented through the utilization of expansive graphs of related words, crafted by state-of-the-art Natural Language Processing (NLP) models. By tapping into the inherent semantic relationships between words, these graphs offer a comprehensive understanding of language nuances.

To implement such dynamic query suggestions, seamless integration with an API becomes imperative. This API facilitates the retrieval of insightful keyword recommendations grounded in the intricate web of semantic connections identified by NLP models. Users benefit from a fluid and intuitive search experience as the search engine anticipates their needs and refines their queries in real-time. This proactive query suggestion mechanism stands as a testament to the evolving landscape of search technology, where the fusion of NLP and intelligent algorithms empowers users with not only accurate search results but also a collaborative and responsive search process.

<br>
<center>
![](./images/related_words_graph.JPG){width=100%}
</center>
<br>

A search interface could retrieve such information via API and display it as follows:

<br>
<center>
![](./images/catalog_search_01.JPG){width=100%}
</center>
<br>

#### Advanced search 

Advanced search are enabled by structured metadata, i.e., by the use of metadata standards and schemas. Accessible through an intuitive user interface, an *advanced search* option provides users with a dynamic and interactive platform to specify their search criteria. Users can pinpoint their focus to distinct metadata fields, enabling a fine-grained exploration of datasets within the catalog. Furthermore, to cater to users familiar with syntax-based queries, the Advanced Search offers a syntax option, granting more adept users the flexibility to craft complex search queries using boolean operators.

Whether through the user-friendly interface or the syntax option, the Advanced Search capability provides a comprehensive and tailored search experience. This feature not only acknowledges the diverse needs of users but also reflects the catalog's ability to facilitating efficient and precise data discovery through the judicious use of structured metadata.

<br>
<center>
![image](https://user-images.githubusercontent.com/35276300/229806372-8c33d0ca-5d3e-48b1-af4f-5a0405c30c22.png){width=85%}
</center>
<br>

This would correspond to the following syntax that the user could enter directly in the search box (and save and/or share with others):
<br>
<center>
**title:"demographic transition" AND country:(*Kenya*) AND body:(poverty)**
</center>
<br>

#### Geographic search

Geographic discoverability and searchability within data catalogs are critical components that cater to the diverse needs of users seeking information pertaining to specific geographic areas. Data catalogs routinely receive a substantial volume of queries centered around particular regions, whether at the country level, sub-national entities, or non-administrative geographic features like watersheds and oceans. In the context of data catalogs housing geographic datasets, it becomes imperative to offer specialized search tools that enhance the precision and relevance of spatial data discovery.

While metadata standards often allow the utilization of bounding boxes to specify geographic coverage, this method has inherent limitations that make it imperfect. Drawing a box on a map to define an area oversimplifies the complexity of geographical features, particularly when users are interested in irregular or non-standard shapes. To address this limitation, it is recommended to incorporate advanced geographic search tools that provide users with more nuanced ways to explore datasets based on their spatial characteristics.

Geographic indexing emerges as a powerful solution for datasets with explicit geographic content. The H3 index, known for its hierarchical hexagonal structure, proves to be a robust option. The hierarchical nature of the H3 index facilitates efficient aggregation and retrieval of data at various levels of granularity, offering an effective means to enhance the geographic discoverability of datasets within a catalog. By adopting geographic indexing, the catalog ensures that users can precisely identify and access datasets relevant to their spatial requirements.

Recognizing that many users rely on keyword searches to discover data, it is also essential to address the challenges associated with incomplete metadata coverage. For instance, a raster image of the Philippines obtained from satellite imagery may contain the country name in its metadata. Still, specific sub-national areas like "Iloilo" may not be explicitly mentioned. To overcome this limitation, a sophisticated search engine should implement intelligent query parsing. This involves detecting if a user query contains the name of a geographic area, automatically identifying the corresponding polygon of geographic coordinates. This process can be facilitated by leveraging APIs such as Nominatim. Subsequently, the search engine retrieves relevant resources in the catalog that cover the specified geographic area, underscoring the importance of geographically indexing datasets to enable accurate and comprehensive results.

This integrated approach ensures that users can seamlessly navigate and explore geographic datasets within the catalog, whether through advanced search tools, geographic indexing, or intelligent keyword parsing, thereby fostering a robust and user-centric data discovery experience.

<br>
<center>
![image](https://user-images.githubusercontent.com/35276300/230091095-d63c8b8f-7684-41db-b347-d75ded1dc95a.png)
</center>
<br>

Example of use of Nominatim: The Nominatim application shows the polygon boundary for the search query “Iloilo City” automatically provided by the API.

<br>
<center>
![image](https://user-images.githubusercontent.com/35276300/230091354-b44c38fa-f628-4693-97bb-f49fb4f23b3e.png)
</center>
<br>

The search API endpoint of Nominatim returns this JSON data which can be processed to generate search cell(s).

<br>
<center>
![image](https://user-images.githubusercontent.com/35276300/230091598-fee71949-29d2-4bac-b60f-dd8efb49278f.png)
</center>
<br>

#### Ranking results 

A search engine must not only identify relevant datasets but also present them in an ordered list based on their level of relevance. The ranking of results directly influences users' ability to quickly find pertinent information, with the most relevant outcomes prominently positioned at the top. If users do not encounter relevance in the initial results, there's a risk of them seeking data from alternative sources, emphasizing the critical role of effective result ranking in user satisfaction and retention.

Achieving optimal result ranking hinges on the careful consideration of both the content and structure of metadata. Relevance engineering plays a pivotal role in this process, requiring fine-tuning of advanced search tools such as Solr or ElasticSearch. This is a crucial step in ensuring that the search engine not only identifies relevant datasets but also presents them in a manner that aligns with users' expectations.

Optimizing keyword-based searches is integral to effective result ranking. While out-of-the-box solutions like those provided by SQL databases may fall short, advanced indexing tools such as Solr and ElasticSearch offer robust mechanisms for enhancement. Structured metadata proves to be a key ally in this optimization effort, enabling the boosting of specific metadata elements. For instance, a query term found in the title holds greater weight than if found in notes, and results are ranked accordingly. Similarly, attributing more weight to a country name found in designated metadata elements like nation or reference country ensures that results align with the contextual relevance of the query.

Furthermore, advanced indexing tools provide additional functionalities, including grouping, sequencing, and proximity-based considerations, further refining the result ranking process. The boosting capabilities offered by tools like Solr and ElasticSearch empower search engines to adapt and align with the nuanced preferences of users, ultimately contributing to an enhanced and efficient data discovery experience.

For large data catalogs managed by well-resourced agencies, the exploration of machine learning solutions, particularly "learn-to-rank" techniques, offers an avenue for result ranking enhancement. This involves leveraging the expertise of data scientists to develop models that can intelligently weigh and prioritize results based on relevance, contributing to an improved and personalized user experience. 

#### Saving and sharing results

Empowering users with the ability to save, export, and share their search queries within a data catalog search engine enhances the overall utility and collaborative potential of the platform. The option to save a search query for future re-use provides users with a convenient way to revisit and refine their information retrieval strategies. This is particularly beneficial for users engaging in recurrent or complex data exploration, allowing them to streamline their workflow and save time.

The capability to export search results is equally crucial, offering users the flexibility to analyze and manipulate the obtained data outside the confines of the search engine. Exported results, whether in the form of downloadable files or accessible URLs, enable seamless integration with external tools and applications for further analysis or visualization.

Sharing functionalities contribute to the collaborative aspect of data exploration. Enabling users to share both their saved queries and the results with others fosters knowledge exchange and collaboration within the user community. The option to share via social media platforms adds an additional layer of accessibility, allowing users to disseminate valuable insights to a broader audience and potentially connect with others who share similar interests or objectives.

In essence, the ability to save, export, and share search queries and results not only enhances the individual user experience by facilitating efficient workflow management but also promotes collaboration, knowledge-sharing, and community engagement within the data catalog ecosystem. This collaborative dimension contributes to the creation of a more dynamic and interactive user community, fostering a culture of shared insights and collective learning.

### Filtering

The incorporation of filtering options, such as facets or other navigational tools, alongside a search engine within a data catalog is of paramount importance. While search engines excel at pinpointing specific items based on user queries, filtering options complement this functionality by enabling users to refine and explore the catalog comprehensively. These filters empower users to navigate through extensive datasets with precision, allowing them to narrow down results based on criteria such as data type, date range, or specific attributes. This not only streamlines the browsing experience for users seeking specific subsets of information but also facilitates a more nuanced exploration of the entire catalog. By offering both a powerful search engine and versatile filtering mechanisms, a data catalog ensures that users can efficiently uncover relevant data through tailored search queries or by dynamically refining their exploration of the complete catalog or search results.

#### Facets 

Facets, also referred to as *filters*, play a crucial role in enhancing the usability of a data catalog by allowing users to refine their searches and navigate through datasets with precision. Structured metadata and controlled vocabularies form the backbone of these facets, providing a systematic framework for categorizing and organizing information. For example, in a data catalog featuring datasets from various countries, a "country" facet can serve as a powerful filter, enabling users to swiftly locate relevant datasets associated with specific geographic regions. The effectiveness of these filters hinges on metadata elements characterized by a limited number of categories and a predictable set of options, which is facilitated through the use of controlled vocabularies.

In the user interface (UI) of a data catalog, the visibility and accessibility of activated filters are paramount. UI design should prioritize making activated filters easily noticeable and providing a straightforward mechanism for deactivation, such as the use of filter "pills" that can be conveniently toggled on and off. This ensures that users have a transparent and intuitive means of managing their refined search criteria. Additionally, recognizing that certain metadata elements are inherently tied to specific data types, the integration of contextual facets within the catalog's UI becomes essential. These contextual facets dynamically offer relevant filters based on the type of data being searched, further optimizing the user experience and facilitating efficient exploration of the catalog's diverse datasets.

<center>
![](./images/catalog_facets_01.JPG){width=100%}
</center>
<br>

Tags and tag groups (which are available in all schemas we recommend) provide much flexibility to implement facets, as we showed in section 1.7. 

#### Organizing entries by collections

Organizing data assets into virtual collections within catalogs significantly enhances users' ability to navigate and discover relevant information efficiently. These collections, which can be structured thematically, geographically, by sponsor, producer, or any other relevant criteria, serve as intuitive groupings (and filters) that streamline the search process. For instance, datasets may be grouped by the origin of the data, geographic coverage, or thematic focus. The virtual nature of collections allows a single data asset to belong to multiple collections simultaneously, recognizing the multidimensional nature of datasets. Each collection must come with a brief descriptive overview, providing users with valuable context about the content it encompasses. 

#### Organizing entries by data type

Organizing data assets within a data catalog by main data type facilitates user navigation and ensures that individuals can easily locate the specific information they seek. In a catalog encompassing diverse data types such as microdata, indicators, geographic datasets, documents, and more, offering a streamlined mechanism for users to filter and display query results by data type is paramount. This functionality accommodates the varied needs of users conducting searches with distinct objectives.

Consider a scenario where users are searching for "US population." One user may be interested solely in obtaining the total population of the USA, while another might specifically seek the public use census microdata sample, and a third may be looking for a relevant publication. To cater to these diverse needs, presenting query results in type-specific tabs (with an "All" option) or providing a filter (facet) by type becomes instrumental. This approach empowers users to focus on the particular data types that align with their requirements, streamlining the information retrieval process.

By implementing such features, data catalogs not only enhance user experience but also cater to the varying capacities and expertise of users. For example, users without statistical analysis expertise may prefer to filter out microdata, focusing only on data types that align with their capabilities. This organization and filtering by data type contribute to a more personalized and user-centric service, ensuring that individuals can efficiently access and utilize the data most relevant to their specific needs and competencies.

<center>
![](./images/catalog_tabs_01.JPG){width=100%}
</center>
<br>

#### Filtering by access type 

Organizing data assets within a data catalog by mode of access and access policy also contributes to ensuring that users can easily identify and access datasets aligned with their specific needs, preferences, and compliance requirements. The mode of access, whether through bulk download, API, or other means, serves as a critical factor for users who may have distinct preferences or technical capabilities. For instance, some users might prefer bulk download options for offline analysis, while others may seek real-time access through APIs.

Equally important is the categorization of datasets based on access policies. Clear and transparent access policies, preferably articulated through a standard license and utilizing a controlled vocabulary, provide users with essential information regarding the type of access granted. This could include distinctions between open access, public use files, licensed access, or access within a secure data enclave. The use of controlled vocabulary enables the creation of a facet (filter) that allows users to refine their search based on specific access conditions.

Making access policies prominently visible in the data catalog is essential for user understanding and compliance. A standardized presentation, potentially through a facet, allows users to quickly filter datasets based on their preferred level of access and usage rights. Additionally, any specific access conditions, restrictions, or requirements should be clearly communicated to users to ensure transparency and adherence to relevant legal and ethical considerations. In this way, organizing data assets by mode of access and access policy not only simplifies the user's navigation of the catalog but also contributes to a more informed and responsible use of the available datasets.

<center>
![](./images/catalog_access_policy_01.JPG){width=100%}
</center>
<br>

### Browsing

Browsing a data catalog refers to the process of exploring and navigating through the catalog's contents to discover relevant datasets or information. Unlike targeted searches that involve specific queries, browsing involves a more exploratory approach, allowing users to casually peruse the catalog's offerings. It is akin to strolling through the aisles of a library or scanning the shelves of a bookstore to see what catches your interest.

In the context of a data catalog, browsing typically involves navigating through categories, collections, or filters that organize datasets based on thematic, geographic, or other criteria. Users might scroll through a list of datasets, explore collections related to specific topics, or filter results based on certain attributes. The goal is to serendipitously come across valuable data assets, discover new insights, or identify datasets that align with the user's interests or project requirements.

Effective browsing in a data catalog often relies on well-organized metadata, clear categorization, and intuitive user interfaces. Browsing functionality can encompass various features such as filters, facets, and sorting options, enabling users to refine their exploration and focus on datasets that match their criteria. Ultimately, the browsing experience in a data catalog is designed to be user-friendly, encouraging users to efficiently explore and discover the wealth of information available within the catalog's repository.

#### Providing core information in the listing page

Designing a data catalog with a concise and informative listing page, presented in the form of a card, is crucial for optimizing user experience and aiding efficient decision-making. This card format should encapsulate key details such as entry title, period, geographic coverage, producer, API availability, a brief description, and potentially a logo. By encapsulating these essential elements in a compact display, users can swiftly evaluate the relevance of a dataset without the need to navigate to a separate page.

The entry title serves as a focal point, providing a quick overview of the dataset's main subject. Period and geographic coverage offer immediate insights into the temporal and spatial dimensions of the data. Knowing the producer lends credibility and context to the dataset, while information on API availability indicates the level of accessibility and functionality for technically inclined users. A concise description complements these details, offering a snapshot of the dataset's purpose and content. Including a logo enhances visual recognition, especially for datasets associated with specific organizations or brands.

This streamlined presentation ensures that users can make informed decisions about exploring further details without the need to open multiple pages. The goal is to empower users with enough information upfront, streamlining their browsing experience and enabling efficient decision-making regarding the datasets most relevant to their needs. Ultimately, the catalog card functions as a powerful tool to engage users, providing a snapshot of critical information and enticing them to delve deeper into datasets that align with their specific interests and requirements.

#### Highlights and most popular entries

Some catalog entries may be highlighted in the catalog, and the most popular entries my be listed in the catalog's home page.

#### Recent additions and history

Providing users with a list of the "latest additions" and maintaining a history of additions and modifications in a data catalog is highly useful for several reasons.

- Stay Updated: The "latest additions" feature allows users to quickly identify and access the most recently added datasets. This is particularly valuable for users who want to stay up-to-date with the latest information or who are interested in newly available datasets that might be relevant to their work or research.

- Track Changes: A history of additions and modifications provides transparency and accountability. Users can track changes made to datasets over time, including updates, revisions, or corrections. This history is crucial for maintaining data integrity and ensuring users are aware of any changes that may impact their use of the data.

- Version Control: For datasets that undergo multiple versions or updates, a history log helps users understand the evolution of the data. This version control is essential for researchers, analysts, or anyone relying on specific versions of datasets for consistency and reproducibility in their work.

- Enhance Trust: Transparency in the catalog's modification history builds trust among users. Knowing when and how datasets have been added or modified contributes to the credibility of the catalog and the reliability of the data it contains.

<br>
![image](https://user-images.githubusercontent.com/35276300/231492091-a96d4c5c-c461-4b5f-88c1-f8db26daa98d.png)
<br>

#### Metadata display and formats

To make metadata easily accessible to users, it is important to display it in a convenient way. The display of metadata will vary depending on the data type being used, as each type uses a specific metadata schema. For online catalogs, style sheets can be utilized to control the appearance of the HTML pages.

In addition to being displayed in HTML format, metadata should be available as electronic files in JSON, XML, and potentially PDF format. Structured metadata provides greater control and flexibility to automatically generate JSON and XML files, as well as format and create PDF outputs. It is important that the JSON and XML files generated by the data catalog comply with the underlying metadata schema and are properly validated. This ensures that the metadata files can be easily and reliably reused and repurposed.

<center>
![](./images/catalog_display_01.JPG){width=100%}
</center>

<br>

#### Variable-level comparison

This features is specific to microdata. E-commerce platforms commonly allow customers to compare products by displaying their pictures and descriptions side-by-side. Similarly, for data users, the ability to compare datasets can be valuable to evaluate the consistency or comparability of a variable or an indicator over time or across sources and countries. However, to implement this functionality, detailed and structured metadata at the variable level are necessary. These metadata standards, such as DDI and ISO 19110/19139, enable the implementation of this feature. In the example below, we show how a query for *water* returns not only a list of seven datasets, but also a list of variables in each dataset that match the query.

<center>
![](./images/catalog_variable_view_01.JPG){width=100%}
</center>
<br>

The *variable view* shows that a total of 90 variables match the searched keyword.

<center>
![](./images/catalog_variable_view_02.JPG){width=100%}
</center>
<br>

After selecting the variables of interest, users should be able to display their metadata in a format that facilitates comparison. The availability of detailed metadata is crucial to ensure the quality and usefulness of these comparisons. For example, when working with a survey dataset, capturing information on the variable universe, categories, questions, interviewer instructions, and summary statistics would be ideal. This comprehensive metadata will enable users to make informed decisions about which variables to use and how to analyze them.

<center>
![](./images/catalog_variable_view_03.JPG){width=100%}
</center>
<br>

#### Image mosaic view

If the catalog contains entries of type "image", a mosaic view should be provided in addition to the listing view. The availability of a mosaic view in an image catalog offers users a highly convenient and visually intuitive way to explore and navigate through a large collection of images. A mosaic view typically presents a grid or tiled layout of images, allowing users to see multiple thumbnails simultaneously on a single screen.

Here are several conveniences for users provided by a mosaic view in an image catalog:

   - Efficient visual scanning: A mosaic view enables users to efficiently scan and preview a large number of images at once. This is particularly valuable when users are seeking specific images or trying to get a quick overview of the content available in the catalog.
   - Rapid identification: Users can quickly identify images of interest based on their visual content without the need to open individual files. The mosaic layout provides a holistic view, aiding in the rapid identification of relevant images.
   - Contextual understanding: Seeing images side by side in a mosaic view allows users to grasp contextual relationships and patterns that may not be apparent when viewing images individually. This is particularly beneficial when exploring thematic or chronological relationships among images.
   - Streamlined selection Process: Users can efficiently select multiple images for further actions, such as downloading, sharing, or organizing. This streamlines the selection process, enhancing user productivity and reducing the time required to interact with the catalog.
   - Enhanced user experience: The visual appeal of a mosaic view enhances the overall user experience. It provides an engaging and interactive way for users to interact with the image catalog, making the exploration of images more enjoyable and user-friendly.

The images should be organized into albums.

#### Data preview

The inclusion of a data preview feature in a data catalog is convenient for users as it allows them to get a quick glimpse of the dataset's content without having to download or open the entire file. This feature is particularly beneficial for assessing data relevance, structure, and quality before committing to a full download or exploration.

For tabular data, such as indicators or microdata, the convenience is further amplified by various options to display the data in user-friendly data grids. These grids present the tabular data in a structured and readable format, making it easy for users to interpret and analyze the information. Additionally, the preview feature often accommodates customization options, allowing users to choose specific columns or rows for display, focusing on the most relevant aspects of the data.

In the case of large datasets, a preview feature typically offers a sample of the data rather than the entire dataset. This is practical for several reasons. First, it provides users with a representative subset of the data, offering insights into its characteristics without overwhelming them with excessive information. Second, displaying a sample helps manage resource constraints associated with loading and rendering large datasets, ensuring a smooth and responsive user experience.

The implementation of such a feature requires that the data be available via an API.

<center>
![](./images/catalog_data_preview_01.JPG){width=80%}
</center>

For data of type "document", the "data preview" would consist of a document viewer that would allow the user to view the document within the application (even when the document is not stored in the catalog itself but in an external website). When implementing such a feature, check that the terms of use of the origination source allows that.

<br>
<center>
![image](https://user-images.githubusercontent.com/35276300/230733447-55c75dbb-5e5c-4788-9e58-ae4fca646a85.png)
</center>
<br>

For data of type "video", an option to play the video in the catalog itself should be provided. Typically, this will require publishing the video in an on-line service like Youtube, and embedding it into the catalog page.

#### Visualizations

Embedding visualizations in a data catalog can greatly enhance its usefulness. Different types of data require different types of visualizations. For instance, time series data can be effectively displayed using a line chart, while images with geographic information can be displayed on a map that shows the location of the image capture. For more complex data, other types of charts can be created as well. However, in order to embed dynamic charts in a catalog page, the data needs to be available via API. A good data catalog should offer flexibility in the types of charts and maps that can be embedded in a metadata page. For instance, the NADA catalog provides catalog administrators with the ability to create visualizations using various tools. By including visualizations in a data catalog, users are able to quickly and easily understand the data and gain insights from it.

The NADA catalog allows catalog adinistrators to generate such visualizations using different tools of their choice. The example below were generated using the open-source [Apache eCharts](https://echarts.apache.org/en/index.html) library.

<br>
*Example: Line chart for a time series*

<center>
![](./images/catalog_visualization_03.JPG){width=100%}
</center>

<br>
*Example: Geo-location of an image*

<center>
![](./images/catalog_visualization_05.JPG){width=100%}
</center>
<br>

#### Information on API access



#### Showing links 

Not all data catalog users know exactly what they are looking for and may need to explore the catalog to find relevant resources. E-commerce platforms use recommender systems to suggest products to customers, and data catalogs should have a similar commitment to bringing relevant resources to users' attention. To achieve this, modern data catalogs display relationships between entries, which may involve data of different types, such as microdata files, analytical scripts, and working papers.

These relationships can be documented in the metadata, such as identifying datasets as part of a series or new versions of a previous dataset. When relationships are not known or documented, machine learning tools such as topic models and word embedding models can be used to establish the topical or semantic closeness between resources of different types. This can be used to implement a recommender system in data catalogs, which automatically identifies and displays related documents and data for a given resource. The image below shows how "related documents" and "related data" can be automatically identified and displayed for a resource (in this case a document).

<center>
![](./images/catalog_related_01.JPG){width=100%}
</center>
<br>

#### Related entries (recommendations)




### Sorting 

The entries displayed in the user interface must be sortable by some key features, including the title of the entry, the year, the date the entry was added or last modified, the geographic area, and possibly others. To make filters operate properly, the use of controlled vocabulary may be required for some metadata fields, such as the geographic coverage of the data. The country names, for example, should be consistent across the catalog (the Democratic Republic of Congo should not be named "Democratic Republic of Congo" in some cases and "Congo, Democratic Republic" in others). 


### Other features for users

#### Data and metadata API

To keep up with modern data management needs, a comprehensive data catalog must provide users with convenient access to both data and metadata through an application programming interface (API). The structured metadata in a catalog allows users to extract specific components of the metadata they need, such as the identifier and title of all microdata and geographic datasets conducted after a certain year. With an API, users can easily and automatically access datasets or subsets of datasets they require. This enables internal features of the catalog such as dynamic visualizations and data previews, making data management more efficient. It is crucial that detailed documentation and guidelines on the use of the data and metadata API are provided to users to maximize the benefits of this feature.

Metadata (and data) should be accessible via API
If data accesisble via API, make it clear (and filter)
The API should be well documented with examples.
API query builder: UI for building an API query
Limitations as required (API key; permission system; monitoring; security) 

#### Bulk download option

Even when UI or visualizations etc are shown, many users just want to downlaod the data and metadata.
(...)

#### Alerts (notifications)

Users may want to be automatically notified (by email) when new entries of interest are added, or when change are made to a specific resource. A system allowing users to set criteria for automatic notification can be developed.

Example of Google SCholar alerts:

<br>
<center>
![image](https://user-images.githubusercontent.com/35276300/230730245-ea3702f6-b877-436a-9833-492afafa0270.png)
</center>
<br>

#### Time series query user interface

For time series only

#### Data and metadata as a service

- Maintain a data service: let external users build dashboards/platorms dynamically connected via API; one organization cannot customize to all communities of users.

Build your own dashboards
- Allow users to set preferences: thematic, data type, geographies, search query
- Have a page where pre-designed dashboards (country/thematic pages) and custom dashboard are accessible
- Allow sharing of dashboards
- Core idea: all data and metadata accessible via API; platform operates as a service to feed dashboards (within the platform or external) 

#### Online data access forms 

Make the process of registration, requests fully digital, easy, and fully traceable.

#### Permanent URLs 

To ensure efficient management and organization of datasets within a data catalog, it is essential to assign a unique identifier to each dataset. This identifier should not only meet technical requirements but also serve other purposes such as facilitating dataset citation. To achieve maximum effectiveness, it is recommended that datasets have a globally unique identifier, which can be accomplished through the assignment of a Digital Object Identifier (DOI). DOIs can be generated in addition to a catalog-specific unique identifier and provide a permanent and persistent identifier for the dataset. For more information about the process of generating DOIs and the reasons to use them, visit the [DataCite website](https://datacite.org/).   

Include a citation requirement in metadata.

#### Archive / tombstone

When a dataset is removed or replaced, the reproducibility of some analysis may become impossible. This may be a problem for some users. Unless there is a reason for not making them accessible, old versions of datasets should be kept accessible. But they should not be the ones indexed and dislayed in the catalog, to avoid cnfusion or the risk that a user would exploit a version other than the latest. Moving datasts that are replaced to an archive section of the catalog (not indexed) is an option. Note that DOIs require a permanent web page.

#### Catalog of citations

A data catalog should not be limited to data. Ideally, the scripts produced by researchers to analyze the data, and the output of their analysis, should also be available. An ideal data catalog will allow a user to:

- search for data, and find/access the related scripts and citations
- search for a document (analytical output), and find/access the related data and scripts
- search for a script, and find/access the data and analytical output

Maintain a catalog of citations of datasets. 

<br>
<center>
![image](https://user-images.githubusercontent.com/35276300/229811421-fbda05da-2390-42c5-815c-5fcbc90d9ee1.png)
</center>
</br>

#### Reproducible and replicable scripts

Document, catalog, and publish reproducible/replicable scripts.

<br>
<center>
![image](https://user-images.githubusercontent.com/35276300/229810244-f68655ee-5173-444a-a4c6-5c2446a5361d.png)
</center>
</br>

#### Users' feedback

Feedback on catalog certainly. In the form of a "Contact" email and possibly a "feedback form". Also, if the platform itself is open source, GitHub for issues and suggestions on the application itself.

BUT: Users forum, "reviews" as in e-commerce platforms, is not always recommended. Not all users are 'constructive" and qualified. Requires moderation, which can be costly and controversial. May create dis-incentives for data producers to publish their data. Could be a good option for data platforms that are internal to an organization (where comments are attributed, and an authentication system controls who can provide feedback), but not for public data platforms.

#### Support

Contact, responsive
FAQs

#### Interaction with resource

Generative AI

#### AI-activated assistance to data analysis 

Writing scripts; copilot



## Features for data providers 

When the data catalog is not administered by the producer of the data but by an entrusted repository, data providers want:

### Safety

- Safety, protection against reputation risk (responsible use of data)
- Guarantee that regulations and terms of use will be strictly complied with; reputation of the organization that manages the catalog (Seal of Approval or other accreditation; properly staffed)

### Visibility

- Visibility to maximize the use of data (including options to share/publicize on social media) - screenshot from data.gov

<br>
<center>
![image](https://user-images.githubusercontent.com/35276300/230095637-85901bdc-857a-4d23-a55c-7f67ffbf7a4a.png)
</center>
</br>

### Efficiency

"do not disturb": low burden of deposit and no burden of serving users (minimum interaction with users; providing detailed metadata helps)

### Information on usage

Monitoring of usage (downloads and citations) to assess demand; reports on this (automatically generated)
API and downloads etc

### Feedback from users

Feedback on quality issues



## Features for catalog administrators

In addition to meeting the needs of its users, a modern data catalog should also offer features that a catalog administrator can appreciate or expect. The features listed below can serve as checklist for choice of an application or development of features. These features may include:

### Data deposit

User friendly interface for data deposit. Compliant with metadata stadards. With embedded quality gateways and clearance procedures.

### Privacy protection

Tools for privacy protection control (e.g., tools to identify direct identifiers)

### Free software

Availability of the application as an open-source software, accompanied by detailed technical documentation

### Security

Robust security measures, such as compatibility with advanced authentication systems, flexible role/profile definitions, regular upgrades and security patches, and accreditation by information security experts

### IT affordability

Reasonable IT requirements, such as shared server operability and sufficient memory capacity

### Ease of maintenance

Ease of upgrading to the latest version

###  Interoperability 

Interoperability with other catalogs and applications, as well as compliance with metadata standards. By publishing metadata across multiple catalogs and hubs, data visibility can be increased, and the service provided to users can be maximized. This requires automation to ensure proper synchronization between catalogs (with only one catalog serving as the "owner" of a dataset), which necessitates interoperability between the catalogs, enabled by compliance with common formats and metadata standards and schemas.

### Flexibility on access policies

Flexibility in implementing data access policies that conform to the specific procedures and protocols of the organization managing the catalog.
Both for downloads, browsing (data preview), and API access 

### API based system for automation and efficiency

Availability of APIs for catalog administration
Easy automation of procedures (harvesting, migration of formats, editing, etc.) This means API-based system.

### Featuring tools

Ability to feature datasets


### Usage monitoring and analytics

Easy activation of usage analytics (using Google Analytics, Omniture, or other)


### Multilingual capability

Multilingual capability, including internationalization of the code and the option for catalog administrators to translate or adapt software translations


### Embedded SEO

Embedded Search Engine Optimization (SEO) procedures


### Widgets and plugins

Ability to use widgets to embed custom charts, maps, and data grids in the catalog


### Feedback to developers

Ability to provide feedback and suggestions to the application developers.
   
 
## Machine learning for a better user experience

In Chapter 1, we emphasized the importance of generating comprehensive metadata and how machine learning can be leveraged to enrich it. Natural language processing (NLP) tools and models, in particular, have been employed to enhance the performance of search engines. By utilizing machine learning models, semantic search engines and recommender systems can be developed to aid users in locating relevant data. Moreover, machine learning can improve the ranking of search results to ensure that the most pertinent results are brought to users' attention. Google, Bing, and other leading search engines have employed machine learning for years. While specialized data catalogs may not have the resources to implement such advanced systems, catalog administrators should explore opportunities to utilize machine learning to enhance their users' experience. Catalogs can make use of external APIs to exploit machine learning solutions without requiring administrators to develop machine learning expertise or train their own models. For instance, APIs can be used to automatically and instantly translate queries or convert queries into embeddings. Ideally, a global community of practice will develop such APIs, including training NLP models, and provide them as a global public good.  


### Improved discoverability

In 2019, Google introduced their NLP model, BERT (Biderectional Encoder Representations from Transformers), as a component of their search engine. Other major companies, such as Amazon, Apple, and Microsoft, are also developing similar models to enhance their search engines. One of the objectives of these companies is to create search engines that can support digital assistants like Siri, Alexa, Cortana, and Hey Google, which operate on a conversational mode and provide answers to users rather than just links to resources. Improving NLP models is a continuous and strategic priority for these companies, as not all answers can be found in textual resources. Google is also conducting research to develop solutions for extracting answers from tabular data.

Specialized data catalogs maintained by data centers, statistical agencies, and other data producers still rely almost exclusively on full-text search engines. The search engine within these catalogs looks for matches between keywords submitted by the user and keywords found in an index, without attempting to understand or improve the user's query. This can result in issues such as misinterpretation of the query, as discussed in Chapter 1, where a search for "dutch disease" may be mistakenly interpreted as a health-related query rather than an economic concept.

The administrators of these specialized data catalogs often lack the resources to develop and implement the most advanced NLP solutions, and should not be required to do so. To assist them in transitioning from keyword-based search systems to semantic search and recommender systems, open solutions should be developed and published, such as pre-trained NLP models, open source tools, and open APIs. This would necessitate the creation and publishing of global public goods, including specialized corpora and the training of embedding models on these corpora, open NLP models and APIs that data catalogs can utilize to generate embeddings for their metadata, query parsers that can automatically improve/optimize queries and convert them into numeric vectors, and guidelines for implementing semantic search and recommender systems using tools like Solr, ElasticSearch, and Milvus.

Simple models created from open source tools and publicly-available documents can provide straightforward solutions. In the example below, we demonstrate how these models can "understand" the concept of "dutch disease" and correctly associate it with relevant economic concepts.

<br>
<center>
![](./images/word_graph_dutch_disease.JPG){width=100%}
</center>
</br>


### Improved results ranking

Effective search engines not only identify relevant resources, but also rank and present them to users in an optimal order of relevance. As highlighted in Chapter 1, [research](https://www.webfx.com/internet-marketing/seo-statistics.html) shows that 75% of search engine users do not click past the first page, emphasizing the importance of ranking and presenting results effectively.

Data catalog administrators face two challenges in improving their search engine performance. Firstly, they need to improve their ranking in search engines such as Google by enriching metadata and embedding metadata compliant with DCAT or schema.org standards on catalog pages. Secondly, they need to improve the ranking of results returned by their own search engines in response to user queries.

Google's success in 1996 was largely attributed to their revolutionary approach to ranking search results called *PageRank*. Since then, they and other leading search engines have invested heavily in improving ranking methodologies with advanced techniques like *RankBrain* (introduced in 2015). These approaches include primary, contextual, and user-specific ranking, which utilize machine learning models referred to as Learn to Rank models. [Lucidworks](https://lucidworks.com/post/abcs-learning-to-rank/) provides a clear description of this approach, noting that "Learning to rank (LTR) is a class of algorithmic techniques that apply supervised machine learning to solve ranking problems in search relevancy. In other words, it’s what orders query results. Done well, you have happy employees and customers; done poorly, at best you have frustrations, and worse, they will never return. To perform learning to rank you need access to training data, user behaviors, user profiles, and a powerful search engine such as SOLR. The training data for a learning to rank model consists of a list of results for a query and a relevance rating for each of those results with respect to the query. Data scientists create this training data by examining results and deciding to include or exclude each result from the data set."

Implementing Learn to Rank models can be challenging for data catalog administrators due to the resource-intensive nature of building the training dataset, fitting models, and implementing them. An alternative solution is to optimize the implementation of Solr or ElasticSearch, which can often contribute significantly to improving the ranking of search results. For more information on the challenge and available tools and methods for relevancy engineering, refer to D. Turnbull and J. Berryman's 2016 publication.

<br>
<center>
![](./images/schema_search_ranking.JPG){width=100%}
</center>
</br>

